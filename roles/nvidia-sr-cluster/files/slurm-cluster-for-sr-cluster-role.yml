# 
# ** IMPORTANT **
#   This file is preconfigured for role nvidia-sr-cluster.
#   Please make sure to copy this file and override this one: config/group_vars/slurm_cluster.yml before executing playbook
#


################################################################################
# Slurm                                                                        #
################################################################################
# Slurm job scheduler configuration
# Playbook: slurm, slurm-cluster, slurm-perf, slurm-perf-cluster, slurm-validation
slurm_version: 20.02.4
slurm_install_prefix: /usr/local
slurm_user_home: /local/slurm
slurm_cluster_name: deepops 
slurm_username: slurm
slurm_password: ReplaceWithASecurePasswordInTheVault
slurm_db_username: slurm
slurm_db_password: AlsoReplaceWithASecurePasswordInTheVault
slurm_max_job_timelimit: INFINITE
#slurm_default_job_timelimit:
slurm_enable_monitoring: false

# Ensure hosts file generation only runs across slurm cluster
hosts_add_ansible_managed_hosts_groups: ["slurm-cluster"]

# Enable Slurm high-availability mode
# NOTE: The location for Slurm saved state needs to reside in a location shared by all Slurm controller nodes
#       Ideally this is on external NFS server and not the primary control node, since its failure
#       would also take down that NFS share
slurm_enable_ha: false
slurm_ha_state_save_location: "/sw/slurm"

# Slurm configuration is auto-generated from templates in the `slurm` role.
# If you want to override any of these files with custom config files, please
# set the following vars to the absolute path of your custom files.
# CUSTOM FOR SCENARIO RUNNER: Setting custom slurm.conf
slurm_conf_template: "../../roles/nvidia-sr-cluster/templates/slurm.conf"
#slurm_cgroup_conf_template: "/path/to/cgroup.conf"
#slurm_gres_conf_template: "/path/to/gres.conf"
#slurm_dbd_conf_template: "/path/to/slurmdbd.conf"
# CUSTOM FOR SCENARIO RUNNER: Updated for allowing X11
slurm_configure: './configure --prefix={{ slurm_install_prefix }} --disable-dependency-tracking --disable-debug --enable-really-no-cray --enable-salloc-kill-cmd --with-hdf5=no --sysconfdir={{ slurm_config_dir }} --enable-pam --with-pam_dir={{ slurm_pam_lib_dir }} --without-shared-libslurm --without-rpath'

#slurm_force_rebuild: yes

# Disable epilog and prolog scripts
slurm_enable_prolog_epilog: false

################################################################################
# Disable unnecessary features                                                        #
################################################################################
slurm_cluster_install_cuda: no
slurm_cluster_install_nvidia_driver: no
slurm_cluster_install_singularity: no
slurm_include_hwloc: no
slurm_include_pmix: no
slurm_manage_gpus: false
slurm_install_lmod: false
slurm_install_hpcsdk: false
slurm_cluster_install_openmpi: false
slurm_install_enroot: false
slurm_install_pyxis: false
allow_user_set_gpu_clocks: no

# Flags for enable/disable of NFS deployment
#  - Set up an NFS server on nfs-server?
slurm_enable_nfs_server: true
#  - Mount NFS filesystems on nfs-clients?
slurm_enable_nfs_client_nodes: true

# Inventory host groups to use for NFS server or clients
nfs_server_group: "slurm-master[0]"
nfs_client_group: "slurm-master[1:],slurm-node"


################################################################################
# Open OnDemand                                                                #
################################################################################
install_open_ondemand: no
ood_install_linuxhost_adapter: no

servername: '{{ ansible_fqdn }}'
httpd_port: 9050
httpd_listen_addr_port:
  - 9050
httpd_use_rewrites: false
node_uri: /node
rnode_uri: /rnode


################################################################################
# Node Health Check                                                            #
################################################################################
slurm_install_nhc: no
slurm_health_check_program: "/usr/sbin/nhc"

# The health check configuration generated by default in DeepOps is pretty
# basic, and most cluster administrators will want to set up more extensive
# NHC configurations with their local site customizations.
# To set a custom file for Ansible to provision to /etc/nhc/nhc.conf, set the
# following var:
#
#nhc_config_template: "/path/to/custom/nhc.conf"
#
# For documentation on the file format, see:
# https://github.com/mej/nhc/blob/master/README.md
