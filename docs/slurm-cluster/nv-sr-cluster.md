NVIDIA DriveSim Scenario Runner Slurm Cluster Deployment Guide
===

## Overview

This guide describes a lightweight framework for configuring and deploying a Slurm cluster with NVIDIA DriveSim Scenario Runner enablement. This allows AV developers to run DriveSim scenarios and evaluators in their CICD pipeline.

This package consists of a playbook, customized Slurm configuration, and a few simple shell scripts that serve as a basic but expandable framework for running Scenario Runner with Slurm.

Imporant details of the design and architecture are provided in the [ScenarioRunnerDeepOps.pdf](/roles/nvidia-sr-cluster/ScenarioRunnerDeepOps.pdf) file. Please review that document carefully before starting a deployment.

## Supported Platforms 

This package can be deployed to any set of servers with DriveSim capabilities.

It has been tested on:

* NVIDIA Constellation HW 1.1 SIL/HIL (and 1.0 in SIL only mode)
* DriveSim compatible workstations on prem and on cloud

## Requirements

* 1 or more DriveSim ready workstations or Constellation server
* 1 deployment system use to run the DeepOps Ansible package (min 4GB RAM, 20GB disk space)
* 1 login server to host the Slurm controller and serve as the central job scheduling node (min 4GB RAMP, 60GB disk space)

Note: this guide assumes the DriveSim servers are already configured to run drivesim workflows as a stand-alone device.

Notes on the requirements for the login server (control plane):

* This node will host the Slurm controller and will serve as the central job scheduling node that holds the state of the cluster. Users or CI/CD process will ssh to “Login node” to submit jobs.
* This system must be on the same network subnet as the constellation nodes.
* Minimum system requirements:
   - 4GB of RAM
   - 60GB of disk space (for Slurm logs)
   - Minimum 4 core CPU
   - Assumption: the login server will be used only for Slurm scheduling. If planning to run additional services or allow user interactivity on the login server, please be sure to adjust requirements as necessary.


* Scaling considerations for login server:
   - Performance: One server is enough for at least 1000 nodes! Slurm is designed to operate significantly large clusters with far more complex scheduling algorithms than what is used for this design. Since only one simulation can run on a server at a time, each node is scheduled as an atomic unit, greatly reducing the overhead necessary for scheduling jobs, and as such there are no scaling concerns by using a single server. For more information on Slurm large clusters see this link: https://slurm.schedmd.com/big_sys.html
   - High Availability: Slurm offers a simple fail-over mechanism that can be configured if constant availability is a requirement. Slurm handles outages very well so unless extreme availability is desired, the cost of the complexity is not worthwhile. The reference will not configure Slurm in HA mode. For more info on Slurm HA look here: https://www.totalcae.com/learn/high-availability-slurm/ and https://slurm.schedmd.com/quickstart_admin.html#HA


## Storage

This design uses three file storage locations for workflow execution:

1. Central storage for Slurm logs and shared scripts.
   This is an NFS share that hosts the ScenarioRunner wrapper scripts and the Slurm output logs.
   The location for this share is in a variable named "sr_share_path": [/roles/nvidia-sr-cluster/defaults/main.yml](/roles/nvidia-sr-cluster/defaults/main.yml)

2. Location for all the Scenario Runner test specification files

3. Location for storing test output and artifacts
   This is the destination where all the artifacts (video, logs, artifacts) generated by a simulation get stored.
   All content is generated on the local node and then copied out to this storage location for review and archival.
   This guide will use AWS S3 but any storage techonology can be used.
   A tool called Rclone is leveraged to simplify copying files to cloud locations. 

   * Space requirements: large files - simulation logs and artifacts (video recordings)
   * Performance requirements:
      - Long running copies will take up time on the Constellation node, so a fast storage device is recommended. An additional copy can be run offline (on a CPU node or VM) to move data into the cloud without blocking the Constellation device.
      - Review “Storage worst-case scenario” for your environment and make sure your storage device has sufficient performance to handle peak usage.



## Docker Registry
A workflow executed by Scenario Runner consists of multiple Docker containers that drive and analyze the simulation. The Docker registry path is defined at the test level inside the Scenario Runner job manifest. This means the cluster is not locked to a specific Docker registry.

Any user that intends to run a Scenario Runner job will need to have a credentials.yaml file in their $HOME/.sr directory in order to get access to the container registry. Please see Scenario Runner documenation for instructions on how to configure Scenario Runner.

Adcice to speed up container download:
- Registry proximity: a registry server in the same network with high bandwidth is ideal when the users are also near the cluster.
- Caching proxy: if the registry is remote and bandwidth a limitation a Docker cache proxy can help. Caveat is the first download of a container would be limited, but consequent runs will benefit from the cached image.


## Installation Steps

### Pre-Deployment Preparation ###

1. Deployment information: Collect the information needed for the install:

   * Login server IP and hostname
   * DriveSim server IPs and hostnames
   * Deployment user credentials (requires sudo permissions)
   * Validation user credentials (regular user with no sudo)
   * Docker registry path and credentials
   * Central storage access credentails

2. Users and groups: Create or identify a deployment user and a validation user and prepare them for deployment.

   Deployment user: This user will run the ansible deployment and can be used to update the cluster in the future. Only this user requires elevated permissions and not the regular users submitting jobs.

   Requirements:
   - IMPORTANT: Linux User ID and Group ID must match on ALL nodes before deployment. This is very important otherwise many things will not work. 
   - SSH access is setup for all the nodes using public keys for automated access.
   - Unrestricted sudo privilege with no password on login node and all constellation nodes

   Note on user creation: DeepOps includes tools for local user creation but are bypassed by default. If you use these tools make sure the GID/UID always matches.

   Validation user: This login will be used to run a validation test at the end of the installation. Future users of the system can be configured with matching permissions and setup.

   Requirements:
   - IMPORTANT: Linux User ID and Group ID must match on login and Constellation nodes before deployment. This is very important otherwise many things will not work.
   - SSH access to login node

   Docker Group ID: In order for Slurm to execute Docker commands the docker GID has to match on all nodes ((The Docker GID is configurable, see roles/nv-sr-cluster/defaults/main.yml file, default set to 999)

3. Deployment system: This will be used to run the Ansible scripts and execute the deployment.
   Requirements:
   - Laptop or linux server
   - Ubuntu 18.04 or 20.04
   - Deployment user with ssh and sudo access to all nodes

4. Login node: This will be the Slurm controller and job scheduler.
   Requirements:
   - Laptop or linux server installed according to HW requirments (listed earlier)
   - Ubuntu 18.04 or 20.04
   - Deployment user with ssh and sudo access to all the nodes

5. DriveSim servers: 
   Requirements:
   - DriveSim capable systems (can be Constellation, workstation, or cloud)
   - System is preconfigured and able to run DriveSim as "stand-alone" node (instructions beyond the scope of this guide)
   - Deployment user with ssh and sudo access to all nodes
   - Validation user for testing at the end

6. Access: Login node and DriveSim servers should have access to:
   - Full network access to each other
   - Access to the central storage
   - Access to Docker registry that includes containers used by Scenario Runner workflow


### Prepare provisioning environment ###

1. On the deployment node clone the DeepOps package to begin

   ```git clone https://github.com/NVIDIA/deepops.git```

2. Set up your provisioning machine.

   DeepOps uses a single provisioning machine to deploy all other software to the cluster. This process may take several minutes as ansible-galaxy roles are downloaded and python packages are installed. For more information on Ansible and why we use it, consult the [Ansible Guide](ANSIBLE.md).

   ```sh
   # Install software prerequisites and copy default configuration
   # Copies ./config.example to ./config, if none exists
   ./scripts/setup.sh
   ```

3. Edit the Ansible inventory

   Edit the Ansible inventory file and verify connectivity to all nodes.

   The DriveSim servers must be listed in the slurm-node group, and the login server must be in the slurm-master group.

   Ansible uses an inventory which outlines the servers in the cluster and a set of group variables which playbooks use to customize deployment. Running `./scripts/setup.sh` in the previous step should have created the `config` directory.
      
   ```sh
   # Modify the Ansible inventory file
   # Especially the `all` and `slurm` sections
   vi config/inventory
   ```

   > NOTE: Be warned that `/etc/hostname` and `/etc/hosts` on each host will be modified to the name(s) specified in the inventory file, so it is best to use the actual names of the hosts.

   When modifying the inventory, if the hosts are not accessible from the provisioning node by their hostname, supply an an `ansible_host`. For example:

   ```yml
   # in config/inventory...

   [all]
   login-node ansible_host_192.168.2.100
   drivesim-node-01 ansible_host=192.168.2.1
   drivesim-node-02 ansible_host=192.168.2.2

   ...

   [slurm-master]
   login-node

   [slurm-node]
   drivesim-node-01
   drivesim-node-02

   ```

4. Verify the configuration

   ```sh
   ansible all -m raw -a "hostname"
   ```

5. Review playbook and edit the configuration files

   It is important to review the playbook and understand each section in order to complete the configuration correctly.

   The playbook is located at [/playbooks/nvidia-sr-cluster.yml](/playbooks/nvidia-sr-cluster.yml). It is divided into sections by tags allowing the install to occur in steps or to rerun an individual section.

   Below is a review of each secion and the relevant configuration files:


   - preinstall: Prepare for deployment and establish connectivity across nodes and user access. 
       The only configuration needed is if the users section is enabled (meaning you intend to create users instead of using a preconfigured environment).
       Configuration variables in config/group_vars/all.yml 

   - docker: Install docker-ce on the login node, docker-compose on the constellation nodes,
       Configure the docker_group_id in config/group_vars/all.yml

   - nfs: Configures NFS shares to be used for slurm log files and wrapper scripts central location.
       The exports are configured in config/group_vars/all.yml in the NFS section.
       By default the variable sr_share is mounted on all DriveSim nodes and is local to the login node.

   - slurm: installs Slurm. Before executing the playbook please make sure to look over the instructions
       The Slurm configuration requires special handling please see Slurm config section below.

   - rclone: Install RCLONE and set up the Central Storage location where all the SR logs and artifacts will be stored.
       The storage location and credentials need to be added to the rclone config.
       For more information on how to configure the rclone role please see the roles/rclone/defaults/main.yml file.

   - scripts: Install wrapper scripts to enable Scenario Runner cluster
       These scripts build a very basic framework for launching Scenario Runner and collecting results.
       To configure please review the default values in the nvidia-sr-cluster role in roles/nvidia-sr-cluster/main.yml.

   - utils: optional helper utilities
       Additonal tools to help with DriveSim and Scenario Runner environment setup


6. Slurm configuration

   Slurm is configured to manage a node as a single unit instead of a bundle of resources. The file slurm.conf is modified from it’s original (roles/slurm/templates/etc/slurm/slurm.conf) to:

   ```
   # Disable cgroups and resource affinity
   TaskPlugin=task/none

   # Disable backfill scheduling
   SchedulerType=sched/builtin

   # Use FIFO linear scheduling alorithm
   SelectType=select/linear

   # Treat each system as a unit
   SelectTypeParameters=CR_ONE_TASK_PER_CORE

   # Avoid using cgroups
   JobAcctGatherType=jobacct_gather/linux
   ```

   IMPORTANT: The nvidia-sr-cluster role comes with a customized yml file that points to slurm.conf template inside the nvidia-sr role and it must be copied to overwrite the slurm-cluster.yml in the config/group_vars folder.

      $ cp roles/nvidia-sr-cluster/files/slurm-cluster-for-sr-cluster-role.yml config/group_vars/slurm-cluster.yml

7. Deployment

   Run the cluster playbook at [/playbooks/nvidia-sr-cluster.yml](/playbooks/nvidia-sr-cluster.yml)

   ```sh
   ansible-playbook -l slurm-cluster playbooks/nvidia-sr-cluster.yml [--tags=tag_name]
   ```

   Note that an individual section can be rerun by using the tag_name.


## Validation

### Prepare user environment

   First we need to prepare the validation user environment with all the credentials. Many commands can be run from the deployment system using ansible ad-hock command line for simplicity.

   IMPORTANT NOTE: If this user does not have a shared $HOME directory across the nodes then some of these steps (like Docker login, and SR credentials) will have to be done individually for every node. That is because Docker and SR credentials are stored in $HOME folder.

   Fist add the user to the docker group on all systems (including the login node) (https://docs.docker.com/engine/install/linux-postinstall/)

   ```sh
   # helper ansible command (replace VALIDATION_USER for actual username)
   ansible all -m shell -a "usermod -aG docker <validation_user>" -vvv --become
   ```

   Test slurm back of simple docker command. Here is an example:

   ```sh
   ssh <validation_user>@<login_node>
   cd $SR_CLUSTER_ROOT_DIR
   sbatch scripts/test_docker.sh
   # output: Submitted batch job <JOB_ID>
   cat slurm-<JOB_ID>.out
   # output: Hello from Docker!
   # output:   This message shows that your installation appears to be working correctly.
   ...

   Log user into Docker registries on DriveSim nodes

   The user needs access to the Docker registries where Scenario Runner, DriveSim, and AV are residing (docker credentials are saved in $HOME/.docker, usually the $HOME directory is shared and credentials only need to be created once, however if your $HOME directories are not shared you will need to login to Docker registries on each node)

   To view which docker registries are needed by the validation test look inside the manifest file here: config/scripts/hil_verified_0.1.11/manifest.yaml

   Create Scenario Runner credentials file: in your home directory create directory .sr and place credentials.yaml with the entries for different docker registries. Credentials file should look like below. Please enter corresponding registry, username and password token.

   Please contact your NVIDIA representative for credentials.

   ```yaml
   CREDENTIALS:
     - registry: 'nvcr.io/<repository>'
       username: '$oauthtoken'
       password: '<token>'
   ```


### Create a Scenario Runner manifest and copy to cloud storage

   Login to a DriveSim node to create a Scenario Runner manifest to copy into the central storage location.

   ```sh
   # Enter the login node
   ssh <valudation_user>@<drivesim_node>
   cd $SR_CLUSTER_ROOT_DIR

   # Load the cluster environment
   source sr_cluster.env

   # Run Scenario Runner "create" command
   # NOTE: The create commmand needs to be run from a DriveSim compatible system with DISPLAY set
   export DISPLAY=:0
   xhost +
   # NOTE: refer to the Scenario Runner documentation for how to create manifests for different environments
   scripts/launch_sr.sh create -p constellation -l hil hil_test1

   # Copy the test directory into central storage location
   rclone copy hil_test1 $SR_CLUSTER_TEST_SPEC_PATH/hil_test1

   # Verify the folder was copied by listing the directories
   rclone lsd $SR_CLUSTER_TEST_SPEC_PATH
   ```

### Run validation test

   Now from the login node we will sbatch a Scenario Runner task on all nodes.


   ```sh
   # Enter login node
   ssh <validation_user>@<login_node>
   cd $SR_CLUSTER_ROOT_DIR
   
   # Check that the nodes are showing up in the cluster (and idle)
   sinfo
   output: PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
   output: batch*       up   infinite      2   idle cc-node[1-2]
   
   # Launch scenario runner on all nodes
   # To select all nodes we will use sbatch --array parameter
   # The --array parameter takes a range starting with 0:
   #      for example “--array 0-1” will launch 2 jobs
   #      and “--array 0-5” will launch 6 jobs
   # Launch as many jobs as there are nodes for the validation
   sbatch --array 0-1 scripts/run_sr.sh hil_test1
   output: Submitted batch job <JOBID>
   
   # To watch log unfolding
   tail -f slurm-<JOBID>.out
   
   # The results will be copied out to the cloud folder $SR_CLUSTER_TEST_OUTPUT_PATH with the name of the job ID and the test folder name
   
   $ rclone lsd SR_CLUSTER_TEST_OUTPUT_PATH
   ```

